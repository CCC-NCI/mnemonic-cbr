# Comparing Teaching Strategies through Misconception Analysis
_A Hybrid Case-Based Reasoning and AI Simulation Framework for Mathematics Education (IJAIED 2025)_

---

## Project Summary

This repository offers the full simulation framework, results, and persona prompt engineering for the IJAIED 2025 article "Comparing Teaching Strategies through Misconception Analysis: A Case-Based AI Framework Using the EEDI Dataset". It simulates five teaching approaches (Socratic, Constructive, Experiential, Rule-Based, Traditional) addressing math misconception using mnemonic-augmented case-based reasoning (CBR) and large language models (GPT-3.5-turbo). All results are based solely on learning outcomes reconstructed from cases of the EEDI Dataset. No real classroom learning outcomes are measured or claimed. The system works with or without LLM support. To use AI-generated teaching interventions, set an OpenAI API key. Without a key, run in CBR-only mode by adding `--no-llm` to a command.



---

## What This Framework Does / Does Not Do

**Does:**<BR>
- Systematically simulates AI-teaching interventions on real student error cases (diagnostics from EEDI dataset)<BR>
- Compares architecture designs (baseline, CBR, AI, hybrid-mnemonic) and five teaching personas<BR>
- Quantifies effects using cross-validation and ablation<BR>
- Documents API, compute, storage costs for institutional scaling<BR>
- Ships complete, reproducible simulation experiments and prompt templates<BR>

**Does Not :**<BR>
- Evaluate real student learning: all "outcomes" are reconstructed from cases, generated by feature-based regression proxies<BR>
- Offer validated pedagogical recommendations<BR>
- Replace the necessity of live classroom studies or piloting<BR>

---

## Dataset & Reconstructed Outcomes

- Uses the EEDI "all_train.csv" dataset (17M student answers, K-8 math, Kaggle CC0)
- **No actual interventions or retention data exists**: outcomes are predictions using

outcome = 0.15 + 0.65 · m_c + 0.20 · c_x + 0.15 · q_d + N(0, 0.02)



Validated on held-out EEDI data ($r = 0.879$), but **DO NOT INTERPRET as real learning gains.** All findings are for simulation/benchmarking only.

---

## Key Results (As in Published Article)

The Hybrid + Mnemonic system achieved the lowest misconception persistence, outperforming the Traditional baseline by 14.1% and Pure AI by 3.7%. Experiential teaching demonstrated the lowest strategy-level persistence, outperforming Traditional teaching by 4.6% and Rule-based instruction by 13.3%. Chunking proved essential in retrieval. All results are based on reconstructed outcomes.

### Performance Metric

All results use **M-Score** (Misconception Score), which represents the persistence of misconceptions after intervention. Lower values indicate better performance (more effective misconception remediation).

### System Architectures (M-Score, lower is better)

| System                 | Mean M-Score     | vs. Baseline    | vs. Pure AI     |
|------------------------|------------------|-----------------|-----------------|
| Traditional Baseline   | 0.6802 ± 0.0084  | --              | --              |
| Pure CBR Baseline      | 0.6273 ± 0.0073  | 7.8%            | --              |
| Pure AI Baseline       | 0.6062 ± 0.0091  | 10.9%           | --              |
| Hybrid + Mnemonic      | 0.5840 ± 0.0068  | 14.1%           | 3.7%            |

### Teaching Personas (M-Score, lower is better)

| Persona      | M-Score          | vs. Rule-based | vs. Traditional | Best Context           |
|--------------|------------------|----------------|-----------------|------------------------|
| Experiential | 0.5768 ± 0.0195  | 13.3%          | 4.6%            | Conceptual learning    |
| Traditional  | 0.6046 ± 0.0241  | 9.1%           | --              | General instruction    |
| Socratic     | 0.6388 ± 0.0268  | 5.7%           | 3.9%            | Critical thinking      |
| Constructive | 0.6452 ± 0.0279  | 6.7%           | 3.0%            | Scaffolded support     |
| Rule-based   | 0.6650 ± 0.0298  | --             | 10.0%           | Procedural skills      |

---

## Mnemonic Technique Ablation (Synergy)

| Configuration            | Score | vs. Full | vs. Baseline | p-value  |
|--------------------------|-------|----------|--------------|----------|
| Full System              | 0.800 | --       | +33.3%       | --       |
| Without Chunking         | 0.500 | -37.5%   | -16.7%       | <0.001   |
| Without Associative      | 0.800 | 0.0%     | +33.3%       | 0.912    |
| Without Retrieval Cues   | 0.800 | 0.0%     | +33.3%       | 0.847    |
| Without Elaboration      | 0.800 | 0.0%     | +33.3%       | 0.891    |
| Baseline (no mnemonic)   | 0.600 | -25.0%   | --           | --       |

_**Chunking is critical; other techniques provide synergistic improvement only when combined.**_

---

## Cost and Computation

- **LLM API**: approx. USD \$0.003 per GPT-3.5-turbo call (late 2025 rates, 200–300 tokens/response)
- **Storage**: ~500 MB per 100,000 cases
- **Inference latency**: ~0.3 seconds/query
- **Typical run**: 200 cases × 4 folds ≈ \$1.60 in API costs
- **Full school**: 1,000 students × 10 interventions/month ≈ \$30/month in API calls

---

## Reproducibility & Quick Start

### Environment

- **Conda**
```bash
conda create -n cbr-ai python=3.9
conda activate cbr-ai
pip install -r requirements.txt
```

- **venv**
```bash
python3 -m venv venv
source venv/bin/activate # macOS/Linux
venv/Scripts/activate.bat # Windows
pip install -r requirements.txt
```

### Dataset Setup

1. Download the EEDI dataset from [Kaggle](https://www.kaggle.com/datasets/alejopaullier/eedi-external-dataset).
2. Extract `all_train.csv` to a `data` folder.

### Running Experiments

- **Main Comparison**
```bash
python code/run_experiments.py --n-cases 200 --use-real-llm
```

- **Ablations**
```bash
python code/run_ablation.py
```

- **View Results**
```bash
python code/visualize_results.py
```

**API Key**

On Mac or Linux (temporary):
```bash
export OPENAI_API_KEY="your-key"
```

If you add this to your .zshrc or .bash_profile, apply with:
```bash
source ~/.zshrc    # or source ~/.bash_profile
```

On Windows (Command Prompt, permanent):
```cmd
setx OPENAI_API_KEY "your-key"
```
Open a new Command Prompt window afterward to use the updated variable.

---

## Project Structure

```
mnemonic-cbr/
├── README.md
├── LICENSE
├── requirements.txt
├── code/
│   ├── cbr/
│   │   ├── mnemonic_augmentation.py
│   │   └── baseline_implementations.py
│   ├── evaluation/
│   │   └── ablation_studies.py
│   ├── run_experiments.py
│   ├── run_ablation.py
│   ├── parameter_sweep.py
│   └── visualize_results.py
├── data/
│   └── all_train.csv                   # Download from Kaggle
└── results/
    ├── cv_results.csv
    ├── ablation_report.json
    └── figures/                         # Generated charts
```

---

## Essential Files and Folders

- `run_experiments.py` — Main experiments (cross-validation, strategy comparison)
- `run_ablation.py` — Ablation testing suite
- `mnemonic_augmentation.py` — CBR + 4 mnemonic techniques
- `baseline_implementations.py` — All system architectures
- `llm_client.py` — API calls to LLMs
- `persona_prompts/` — Teaching agent prompt templates
- `data/`, `results/`, `figures/` — Dataset, logs, output figures

---

## LLM Integration (Production Deployment)

For reproducing published results with real teaching intervention generation:

```python
from cbr.baseline_implementations import HybridMnemonicSystem
from openai import OpenAI

# Initialize with OpenAI API
client = OpenAI(api_key="your-api-key")
hybrid = HybridMnemonicSystem(mnemonic_engine=engine, llm_client=client)

# Generate teaching intervention
result = hybrid.retrieve_and_generate(query)
```

Cost Estimate: ~$2-5 per 200-case experiment with GPT-3.5-turbo

---

## Teaching Personas

| Persona      | Theory               | Distinguishing Feature       |
|--------------|----------------------|-----------------------------|
| Socratic     | Socratic method      | Guided questions, conflict  |
| Constructive | Vygotsky ZPD         | Scaffolding, adaptive help  |
| Experiential | Kolb's learning      | Real-world context, analogy |
| Rule-based   | Direct instruction   | Explicit procedural steps   |
| Traditional  | Classic teaching     | Explanations, worked ex.    |

---

## Methodology

### Mnemonic Techniques

1. **Chunking (Miller, 1956)** - Groups cases using k-means clustering
2. **Associative Networks (Anderson, 1983)** - Creates semantic similarity graph
3. **Retrieval Cues (Tulving, 1974)** - Weights salient features based on outcome correlation
4. **Elaborative Encoding (Craik & Lockhart, 1972)** - Enriches case representations with contextual information

### Similarity Measurement

The similarity computation implements established principles from CBR literature:
- **Population-based normalization**: Features normalized relative to entire case base (Aha et al., 1991)
- **Exponential decay**: Distance-to-similarity conversion following Shepard's law (1987)
- **Weighted features**: Correlation-based feature weighting for outcome prediction
- **Validation**: Systematic evaluation of alternative metrics confirmed exponential decay superiority

### Statistical Validation

- 4-fold cross-validation with stratified sampling
- Bonferroni-corrected significance testing (α = 0.05)
- Effect size reporting (Cohen's d)
- Two-way ANOVA with interaction analysis

---

## Known Issues & Limitations

- **Outcomes are synthetic; no real learning measured.**
- **UK-centric data only (EEDI); results not globally generalizable.**
- **No longitudinal or post-test metrics; only single measurement per case.**
- **Five-feature CBR; full feature engineering and fairness/bias analysis left for future.**
- **Framework is research-oriented; not classroom-validated or educationally certified.**

---

## Bias, Privacy & Ethics

- **Student data is de-identified.**
- **API/system is for research, not deployment, without compliance checking (GDPR, FERPA, etc.).**
- **All components and caveats match the IJAIED 2025 publication.**

---

## Citation

If using this project for publication or research, please cite as:

```bibtex
@article{janetzko2025teaching,
  title={Comparing Teaching Strategies through Misconception Analysis: A Case-Based AI Framework Using the EEDI Dataset},
  author={Janetzko, Dietmar and Gonzalez-Velez, Horacio},
  journal={International Journal of Artificial Intelligence in Education},
  year={2025},
  note={Simulation framework with synthetic outcome proxies.}
}
```

---

## Acknowledgments

- Funded by Erasmus+ 101140316 (digital4sustainability.eu); thanks to EEDI, Kaggle, NCI.
- Contact: Dietmar Janetzko, Horacio Gonzalez-Velez (dietmar.janetzko@ncirl.ie)
- [https://github.com/CCC-NCI/mnemonic-cbr](https://github.com/CCC-NCI/mnemonic-cbr)
- Last updated: 2025-11-08

---

_This simulation framework is for benchmarking and research development: empirical classroom validation is essential before drawing actionable pedagogical conclusions._
